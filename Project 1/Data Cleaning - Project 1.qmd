---
title: "Data Cleaning - Project 1"
author: "Robert Johnson"
date: "`r format(Sys.time(), '%d %B %Y')`"
number-sections: true
number-depth: 3
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
    code-fold: true
# bibliography: dasc-6000.bib 
# csl: ieee-with-url.csl
# linkcolor: red
# urlcolor: blue
# link-citations: yes
# header-includes:
#   - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

# What is an outlier?

The book offers a broad definition for an outlier "an outlier is an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism" (Hawkins 1980). For example, if the average score on an exam was an 80 a grade of 0 would be considered an outlier. Different outlier detection techniques have differing ways in how they determine normal behavior and consequently what is and is not an outlier. Data visualization tools can be helpful for identifying potential outliers and selecting the right outlier detection techniques.

```{r}
#| echo: false
#| output: false
#| label: load packages
library(tidyverse)

#Makes a histogram for each column in the data frame passed to it
#Input: data-frame of continuous functions
#Output: Histogram for each column in the data-frame
make.hist <- function(a) {
  feature <- colnames(a)
  for (i in feature) {
    hist(a[[i]], xlab = i, main = 'Frequency Distribution')
  }
}
```

```{r}

df <- data.frame(Name=c("Vivian Baskette", "Jamison Marney", "Marie Mulero", "Trudi Kimmell",                         "Stephanie Lindemann", "Dia Werley", "Abbie Lama", "Misti Luce",                             "Wilda Byerly"),
                 Age=c(1, 25, 27, 30, 32, 35, 40, 41, 1000),
                 Income=c(70,110,80,130,120,80,90,100,120),
                 Tax=c(7,11,8,13,7,8,9,10,12))
df <- as_tibble(df)
df
```

We can see in the above example the Age feature has two potential outliers that immediately stand out. The other features are not so obvious where outliers may be.

# Outlier Detection Techniques

A basic taxonomy of detection models can be seen as follows (Ilyas et al., 2019)

```{mermaid}
flowchart TD
  Outlier-Detection-Techniques ---> Statistics-Based
  Outlier-Detection-Techniques ---> Distance-Based
  Outlier-Detection-Techniques ---> Model-Based
  Statistics-Based ---> Hypothesis-Testing
  Statistics-Based ---> Fitting-Distribution
  Fitting-Distribution ---> Parametric
  Fitting-Distribution ---> Nonparametric
  Distance-Based ---> Global
  Distance-Based ---> Local
  Model-Based ---> One-Class
  Model-Based ---> Multiple-Class
```

## Statistics-Based

These techniques define a normal value as one that appears in the high probability region of a stochastic model, and an outlier vice-versa.

### Hypothesis Testing

This type of testing can be seen in the Grubbs Test, and Tietjen Moore Test. Typically these methods will define and calculate a test statistic and declare a significance level/critical area. The null hypothesis states that no outliers exist, and the alternative states the opposite. When the test statistic is computed for the observed data if it falls within the critical region the null hypothesis is rejected.

### Model Fitting

Model fitting attempts to fit a known distribution or *pdf* to the data-set. Data-points that have a low probability according to the fitted distribution are declared as outliers.

## Distance-Based

These techniques used the distance between the data-points to define normal behavior. Data-points that are far away from others are assumed to be outliers.

### Global Approach

This approach considers all other data-points when determining distance for each individual point.

### Local Approach

This approach uses a 'neighborhood' that can be defined as needed to determine the distance for each data-point.

## Model-Based

These techniques take advantage of labeled data-sets to train a classifier-model then apply this model to a data-point to determine whether it is normal or not.

### Multi-Class

This approach makes the assumption that the training data contains data that has multiple normal classes

### One-Class

This approach makes the assumption that all of the training data points to one normal class

# PCA

Principal Component Analysis is a popular technique used to analyze large data-sets that have high dimensionality. It reduces the dimensionality of the data-set without losing much information. It works by computing two principal components. The first component is computed so that it explains the most variance in the original features, and the second is computed to explain the most variance left after the first component. This is extremely useful in large datasets as it reduces it down to just a few key features.
